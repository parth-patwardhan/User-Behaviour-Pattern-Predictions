{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOL_2uecbqjP"
      },
      "source": [
        "# **Implementation for User Behaviour Pattern Prediction using Attention Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhwkiZg7nY22",
        "outputId": "0c573d79-7141-49bd-fe2d-5c8ce02f90d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-17 20:39:15--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2025-02-17 20:39:15--  http://snap.stanford.edu/jodie/wikipedia.csv\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 559937473 (534M) [text/csv]\n",
            "Saving to: ‘data/wikipedia.csv’\n",
            "\n",
            "wikipedia.csv       100%[===================>] 534.00M  8.18MB/s    in 48s     \n",
            "\n",
            "2025-02-17 20:40:03 (11.2 MB/s) - ‘data/wikipedia.csv’ saved [559937473/559937473]\n",
            "\n",
            "FINISHED --2025-02-17 20:40:03--\n",
            "Total wall clock time: 48s\n",
            "Downloaded: 1 files, 534M in 48s (11.2 MB/s)\n"
          ]
        }
      ],
      "source": [
        "!wget wget http://snap.stanford.edu/jodie/wikipedia.csv -P data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "vt45ADqnoB96",
        "outputId": "8d764625-29f7-4c07-f3b2-ab79f66cafd5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-792f24c1-bff3-484a-b7d3-97f4a881cb40\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>state_label</th>\n",
              "      <th>comma_separated_list_of_features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0</th>\n",
              "      <th>-0.175063</th>\n",
              "      <th>-0.176678</th>\n",
              "      <th>-0.937091</th>\n",
              "      <th>-0.381926</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.636535</th>\n",
              "      <th>1.052396</th>\n",
              "      <th>-0.16938</th>\n",
              "      <th>-0.19304</th>\n",
              "      <th>-0.169234</th>\n",
              "      <th>-0.828942</th>\n",
              "      <th>-0.175093</th>\n",
              "      <th>-0.239675</th>\n",
              "      <th>-0.081759</th>\n",
              "      <th>-0.438492</th>\n",
              "      <th>-0.265227</th>\n",
              "      <th>-0.270534</th>\n",
              "      <th>-0.766241</th>\n",
              "      <th>-0.230818</th>\n",
              "      <th>-0.208397</th>\n",
              "      <th>-0.031458</th>\n",
              "      <th>-0.146052</th>\n",
              "      <th>-0.125919</th>\n",
              "      <th>-0.114999</th>\n",
              "      <th>-0.073666</th>\n",
              "      <th>-0.080377</th>\n",
              "      <th>-0.039977</th>\n",
              "      <th>-0.086417</th>\n",
              "      <th>-0.07646</th>\n",
              "      <th>-0.115336</th>\n",
              "      <th>-0.112976</th>\n",
              "      <th>-0.129556</th>\n",
              "      <th>-0.114021</th>\n",
              "      <th>-0.126911</th>\n",
              "      <th>-0.112939</th>\n",
              "      <th>-0.08632</th>\n",
              "      <th>-0.12648</th>\n",
              "      <th>-0.140237</th>\n",
              "      <th>-0.127462</th>\n",
              "      <th>-0.101234</th>\n",
              "      <th>-0.12889</th>\n",
              "      <th>-0.12375</th>\n",
              "      <th>-0.036678</th>\n",
              "      <th>-0.132163</th>\n",
              "      <th>-0.091828</th>\n",
              "      <th>-0.076866</th>\n",
              "      <th>-0.083422</th>\n",
              "      <th>-0.132653</th>\n",
              "      <th>-0.132271</th>\n",
              "      <th>-0.113523</th>\n",
              "      <th>-0.082894</th>\n",
              "      <th>-0.100532</th>\n",
              "      <th>-0.075784</th>\n",
              "      <th>-0.143635</th>\n",
              "      <th>-0.127528</th>\n",
              "      <th>-0.128455</th>\n",
              "      <th>-0.099046</th>\n",
              "      <th>-0.112457</th>\n",
              "      <th>-0.118284</th>\n",
              "      <th>-0.105322</th>\n",
              "      <th>-0.117228</th>\n",
              "      <th>-0.105303</th>\n",
              "      <th>-0.130077</th>\n",
              "      <th>-0.117335</th>\n",
              "      <th>-0.100452</th>\n",
              "      <th>-0.082987</th>\n",
              "      <th>-0.109775</th>\n",
              "      <th>-0.082519</th>\n",
              "      <th>-0.09015</th>\n",
              "      <th>-0.061807</th>\n",
              "      <th>-0.076595</th>\n",
              "      <th>-0.154405</th>\n",
              "      <th>-0.133709</th>\n",
              "      <th>-0.149318</th>\n",
              "      <th>-0.147526</th>\n",
              "      <th>-0.157631</th>\n",
              "      <th>-0.133908</th>\n",
              "      <th>-0.142128</th>\n",
              "      <th>-0.101243</th>\n",
              "      <th>-0.094235</th>\n",
              "      <th>-0.084585</th>\n",
              "      <th>-0.089205</th>\n",
              "      <th>-0.05786</th>\n",
              "      <th>-0.067859</th>\n",
              "      <th>-0.055429</th>\n",
              "      <th>-0.052329</th>\n",
              "      <th>-0.085588</th>\n",
              "      <th>-0.084010</th>\n",
              "      <th>1.756454</th>\n",
              "      <th>-0.289939</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.417990</th>\n",
              "      <th>-1.410948</th>\n",
              "      <th>-0.109215</th>\n",
              "      <th>-0.122009</th>\n",
              "      <th>-0.040390</th>\n",
              "      <th>1.825762</th>\n",
              "      <th>-0.120957</th>\n",
              "      <th>-0.175118</th>\n",
              "      <th>-0.062041</th>\n",
              "      <th>-0.265922</th>\n",
              "      <th>-0.108335</th>\n",
              "      <th>-0.118516</th>\n",
              "      <th>1.196213</th>\n",
              "      <th>-0.151928</th>\n",
              "      <th>-0.143451</th>\n",
              "      <th>-0.009262</th>\n",
              "      <th>-0.103378</th>\n",
              "      <th>-0.089099</th>\n",
              "      <th>-0.081998</th>\n",
              "      <th>-0.052605</th>\n",
              "      <th>-0.0557</th>\n",
              "      <th>-0.030846</th>\n",
              "      <th>-0.060957</th>\n",
              "      <th>-0.053846</th>\n",
              "      <th>-0.081624</th>\n",
              "      <th>-0.078441</th>\n",
              "      <th>-0.087999</th>\n",
              "      <th>-0.079429</th>\n",
              "      <th>-0.088813</th>\n",
              "      <th>-0.071986</th>\n",
              "      <th>-0.065387</th>\n",
              "      <th>-0.089995</th>\n",
              "      <th>-0.097408</th>\n",
              "      <th>-0.093064</th>\n",
              "      <th>-0.076343</th>\n",
              "      <th>-0.086655</th>\n",
              "      <th>-0.085351</th>\n",
              "      <th>-0.036412</th>\n",
              "      <th>-0.087365</th>\n",
              "      <th>-0.059447</th>\n",
              "      <th>-0.053389</th>\n",
              "      <th>-0.055685</th>\n",
              "      <th>-0.087141</th>\n",
              "      <th>-0.089251</th>\n",
              "      <th>-0.073396</th>\n",
              "      <th>-0.058741</th>\n",
              "      <th>-0.06799</th>\n",
              "      <th>-0.038718</th>\n",
              "      <th>-0.097449</th>\n",
              "      <th>-0.087578</th>\n",
              "      <th>-0.090969</th>\n",
              "      <th>-0.072003</th>\n",
              "      <th>-0.081238</th>\n",
              "      <th>-0.081747</th>\n",
              "      <th>-0.068767</th>\n",
              "      <th>-0.087579</th>\n",
              "      <th>-0.075662</th>\n",
              "      <th>-0.082675</th>\n",
              "      <th>-0.07205</th>\n",
              "      <th>-0.068462</th>\n",
              "      <th>-0.045573</th>\n",
              "      <th>-0.073261</th>\n",
              "      <th>-0.053124</th>\n",
              "      <th>-0.06232</th>\n",
              "      <th>-0.04988</th>\n",
              "      <th>-0.0506</th>\n",
              "      <th>-0.105394</th>\n",
              "      <th>-0.090395</th>\n",
              "      <th>-0.099816</th>\n",
              "      <th>-0.102888</th>\n",
              "      <th>-0.102903</th>\n",
              "      <th>-0.090115</th>\n",
              "      <th>-0.096068</th>\n",
              "      <th>-0.068136</th>\n",
              "      <th>-0.060835</th>\n",
              "      <th>-0.058394</th>\n",
              "      <td>-0.062079</td>\n",
              "      <td>-0.044674</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.041448</td>\n",
              "      <td>-0.038775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
              "      <th>36.0</th>\n",
              "      <th>0</th>\n",
              "      <th>-0.175063</th>\n",
              "      <th>-0.176678</th>\n",
              "      <th>-0.937091</th>\n",
              "      <th>-0.381926</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.636535</th>\n",
              "      <th>1.052396</th>\n",
              "      <th>-0.16938</th>\n",
              "      <th>-0.19304</th>\n",
              "      <th>-0.169234</th>\n",
              "      <th>-0.828942</th>\n",
              "      <th>-0.175093</th>\n",
              "      <th>-0.239675</th>\n",
              "      <th>-0.081759</th>\n",
              "      <th>-0.438492</th>\n",
              "      <th>-0.265227</th>\n",
              "      <th>-0.270534</th>\n",
              "      <th>-0.766241</th>\n",
              "      <th>-0.230818</th>\n",
              "      <th>-0.208397</th>\n",
              "      <th>-0.031458</th>\n",
              "      <th>-0.146052</th>\n",
              "      <th>-0.125919</th>\n",
              "      <th>-0.114999</th>\n",
              "      <th>-0.073666</th>\n",
              "      <th>-0.080377</th>\n",
              "      <th>-0.039977</th>\n",
              "      <th>-0.086417</th>\n",
              "      <th>-0.07646</th>\n",
              "      <th>-0.115336</th>\n",
              "      <th>-0.112976</th>\n",
              "      <th>-0.129556</th>\n",
              "      <th>-0.114021</th>\n",
              "      <th>-0.126911</th>\n",
              "      <th>-0.112939</th>\n",
              "      <th>-0.08632</th>\n",
              "      <th>-0.12648</th>\n",
              "      <th>-0.140237</th>\n",
              "      <th>-0.127462</th>\n",
              "      <th>-0.101234</th>\n",
              "      <th>-0.12889</th>\n",
              "      <th>-0.12375</th>\n",
              "      <th>-0.036678</th>\n",
              "      <th>-0.132163</th>\n",
              "      <th>-0.091828</th>\n",
              "      <th>-0.076866</th>\n",
              "      <th>-0.083422</th>\n",
              "      <th>-0.132653</th>\n",
              "      <th>-0.132271</th>\n",
              "      <th>-0.113523</th>\n",
              "      <th>-0.082894</th>\n",
              "      <th>-0.100532</th>\n",
              "      <th>-0.075784</th>\n",
              "      <th>-0.143635</th>\n",
              "      <th>-0.127528</th>\n",
              "      <th>-0.128455</th>\n",
              "      <th>-0.099046</th>\n",
              "      <th>-0.112457</th>\n",
              "      <th>-0.118284</th>\n",
              "      <th>-0.105322</th>\n",
              "      <th>-0.117228</th>\n",
              "      <th>-0.105303</th>\n",
              "      <th>-0.130077</th>\n",
              "      <th>-0.117335</th>\n",
              "      <th>-0.100452</th>\n",
              "      <th>-0.082987</th>\n",
              "      <th>-0.109775</th>\n",
              "      <th>-0.082519</th>\n",
              "      <th>-0.09015</th>\n",
              "      <th>-0.061807</th>\n",
              "      <th>-0.076595</th>\n",
              "      <th>-0.154405</th>\n",
              "      <th>-0.133709</th>\n",
              "      <th>-0.149318</th>\n",
              "      <th>-0.147526</th>\n",
              "      <th>-0.157631</th>\n",
              "      <th>-0.133908</th>\n",
              "      <th>-0.142128</th>\n",
              "      <th>-0.101243</th>\n",
              "      <th>-0.094235</th>\n",
              "      <th>-0.084585</th>\n",
              "      <th>-0.089205</th>\n",
              "      <th>-0.05786</th>\n",
              "      <th>-0.067859</th>\n",
              "      <th>-0.055429</th>\n",
              "      <th>-0.052329</th>\n",
              "      <th>-0.114147</th>\n",
              "      <th>-0.115286</th>\n",
              "      <th>-0.649615</th>\n",
              "      <th>-0.289939</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.417990</th>\n",
              "      <th>0.723261</th>\n",
              "      <th>-0.109215</th>\n",
              "      <th>-0.122009</th>\n",
              "      <th>-0.109234</th>\n",
              "      <th>-0.559786</th>\n",
              "      <th>-0.120957</th>\n",
              "      <th>-0.175118</th>\n",
              "      <th>-0.062041</th>\n",
              "      <th>-0.265922</th>\n",
              "      <th>-0.155394</th>\n",
              "      <th>-0.157044</th>\n",
              "      <th>-0.515898</th>\n",
              "      <th>-0.151928</th>\n",
              "      <th>-0.143451</th>\n",
              "      <th>-0.009262</th>\n",
              "      <th>-0.103378</th>\n",
              "      <th>-0.089099</th>\n",
              "      <th>-0.081998</th>\n",
              "      <th>-0.052605</th>\n",
              "      <th>-0.0557</th>\n",
              "      <th>-0.030846</th>\n",
              "      <th>-0.060957</th>\n",
              "      <th>-0.053846</th>\n",
              "      <th>-0.081624</th>\n",
              "      <th>-0.078441</th>\n",
              "      <th>-0.087999</th>\n",
              "      <th>-0.079429</th>\n",
              "      <th>-0.088813</th>\n",
              "      <th>-0.071986</th>\n",
              "      <th>-0.065387</th>\n",
              "      <th>-0.089995</th>\n",
              "      <th>-0.097408</th>\n",
              "      <th>-0.093064</th>\n",
              "      <th>-0.076343</th>\n",
              "      <th>-0.086655</th>\n",
              "      <th>-0.085351</th>\n",
              "      <th>-0.036412</th>\n",
              "      <th>-0.087365</th>\n",
              "      <th>-0.059447</th>\n",
              "      <th>-0.053389</th>\n",
              "      <th>-0.055685</th>\n",
              "      <th>-0.087141</th>\n",
              "      <th>-0.089251</th>\n",
              "      <th>-0.073396</th>\n",
              "      <th>-0.058741</th>\n",
              "      <th>-0.06799</th>\n",
              "      <th>-0.038718</th>\n",
              "      <th>-0.097449</th>\n",
              "      <th>-0.087578</th>\n",
              "      <th>-0.090969</th>\n",
              "      <th>-0.072003</th>\n",
              "      <th>-0.081238</th>\n",
              "      <th>-0.081747</th>\n",
              "      <th>-0.068767</th>\n",
              "      <th>-0.087579</th>\n",
              "      <th>-0.075662</th>\n",
              "      <th>-0.082675</th>\n",
              "      <th>-0.07205</th>\n",
              "      <th>-0.068462</th>\n",
              "      <th>-0.045573</th>\n",
              "      <th>-0.073261</th>\n",
              "      <th>-0.053124</th>\n",
              "      <th>-0.06232</th>\n",
              "      <th>-0.04988</th>\n",
              "      <th>-0.0506</th>\n",
              "      <th>-0.105394</th>\n",
              "      <th>-0.090395</th>\n",
              "      <th>-0.099816</th>\n",
              "      <th>-0.102888</th>\n",
              "      <th>-0.102903</th>\n",
              "      <th>-0.090115</th>\n",
              "      <th>-0.096068</th>\n",
              "      <th>-0.068136</th>\n",
              "      <th>-0.060835</th>\n",
              "      <th>-0.058394</th>\n",
              "      <td>-0.062079</td>\n",
              "      <td>-0.044674</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.041448</td>\n",
              "      <td>-0.038775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77.0</th>\n",
              "      <th>0</th>\n",
              "      <th>-0.175063</th>\n",
              "      <th>-0.176678</th>\n",
              "      <th>-0.937091</th>\n",
              "      <th>-0.381926</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.636535</th>\n",
              "      <th>1.052396</th>\n",
              "      <th>-0.16938</th>\n",
              "      <th>-0.19304</th>\n",
              "      <th>-0.169234</th>\n",
              "      <th>-0.828942</th>\n",
              "      <th>-0.175093</th>\n",
              "      <th>-0.239675</th>\n",
              "      <th>-0.081759</th>\n",
              "      <th>-0.438492</th>\n",
              "      <th>-0.265227</th>\n",
              "      <th>-0.270534</th>\n",
              "      <th>-0.766241</th>\n",
              "      <th>-0.230818</th>\n",
              "      <th>-0.208397</th>\n",
              "      <th>-0.031458</th>\n",
              "      <th>-0.146052</th>\n",
              "      <th>-0.125919</th>\n",
              "      <th>-0.114999</th>\n",
              "      <th>-0.073666</th>\n",
              "      <th>-0.080377</th>\n",
              "      <th>-0.039977</th>\n",
              "      <th>-0.086417</th>\n",
              "      <th>-0.07646</th>\n",
              "      <th>-0.115336</th>\n",
              "      <th>-0.112976</th>\n",
              "      <th>-0.129556</th>\n",
              "      <th>-0.114021</th>\n",
              "      <th>-0.126911</th>\n",
              "      <th>-0.112939</th>\n",
              "      <th>-0.08632</th>\n",
              "      <th>-0.12648</th>\n",
              "      <th>-0.140237</th>\n",
              "      <th>-0.127462</th>\n",
              "      <th>-0.101234</th>\n",
              "      <th>-0.12889</th>\n",
              "      <th>-0.12375</th>\n",
              "      <th>-0.036678</th>\n",
              "      <th>-0.132163</th>\n",
              "      <th>-0.091828</th>\n",
              "      <th>-0.076866</th>\n",
              "      <th>-0.083422</th>\n",
              "      <th>-0.132653</th>\n",
              "      <th>-0.132271</th>\n",
              "      <th>-0.113523</th>\n",
              "      <th>-0.082894</th>\n",
              "      <th>-0.100532</th>\n",
              "      <th>-0.075784</th>\n",
              "      <th>-0.143635</th>\n",
              "      <th>-0.127528</th>\n",
              "      <th>-0.128455</th>\n",
              "      <th>-0.099046</th>\n",
              "      <th>-0.112457</th>\n",
              "      <th>-0.118284</th>\n",
              "      <th>-0.105322</th>\n",
              "      <th>-0.117228</th>\n",
              "      <th>-0.105303</th>\n",
              "      <th>-0.130077</th>\n",
              "      <th>-0.117335</th>\n",
              "      <th>-0.100452</th>\n",
              "      <th>-0.082987</th>\n",
              "      <th>-0.109775</th>\n",
              "      <th>-0.082519</th>\n",
              "      <th>-0.09015</th>\n",
              "      <th>-0.061807</th>\n",
              "      <th>-0.076595</th>\n",
              "      <th>-0.154405</th>\n",
              "      <th>-0.133709</th>\n",
              "      <th>-0.149318</th>\n",
              "      <th>-0.147526</th>\n",
              "      <th>-0.157631</th>\n",
              "      <th>-0.133908</th>\n",
              "      <th>-0.142128</th>\n",
              "      <th>-0.101243</th>\n",
              "      <th>-0.094235</th>\n",
              "      <th>-0.084585</th>\n",
              "      <th>-0.089205</th>\n",
              "      <th>-0.05786</th>\n",
              "      <th>-0.067859</th>\n",
              "      <th>-0.055429</th>\n",
              "      <th>-0.052329</th>\n",
              "      <th>-0.114147</th>\n",
              "      <th>-0.115286</th>\n",
              "      <th>-0.649615</th>\n",
              "      <th>-0.289939</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.417990</th>\n",
              "      <th>0.723261</th>\n",
              "      <th>-0.109215</th>\n",
              "      <th>-0.122009</th>\n",
              "      <th>-0.109234</th>\n",
              "      <th>-0.559786</th>\n",
              "      <th>-0.120957</th>\n",
              "      <th>-0.175118</th>\n",
              "      <th>-0.062041</th>\n",
              "      <th>-0.265922</th>\n",
              "      <th>-0.155394</th>\n",
              "      <th>-0.157044</th>\n",
              "      <th>-0.515898</th>\n",
              "      <th>-0.151928</th>\n",
              "      <th>-0.143451</th>\n",
              "      <th>-0.009262</th>\n",
              "      <th>-0.103378</th>\n",
              "      <th>-0.089099</th>\n",
              "      <th>-0.081998</th>\n",
              "      <th>-0.052605</th>\n",
              "      <th>-0.0557</th>\n",
              "      <th>-0.030846</th>\n",
              "      <th>-0.060957</th>\n",
              "      <th>-0.053846</th>\n",
              "      <th>-0.081624</th>\n",
              "      <th>-0.078441</th>\n",
              "      <th>-0.087999</th>\n",
              "      <th>-0.079429</th>\n",
              "      <th>-0.088813</th>\n",
              "      <th>-0.071986</th>\n",
              "      <th>-0.065387</th>\n",
              "      <th>-0.089995</th>\n",
              "      <th>-0.097408</th>\n",
              "      <th>-0.093064</th>\n",
              "      <th>-0.076343</th>\n",
              "      <th>-0.086655</th>\n",
              "      <th>-0.085351</th>\n",
              "      <th>-0.036412</th>\n",
              "      <th>-0.087365</th>\n",
              "      <th>-0.059447</th>\n",
              "      <th>-0.053389</th>\n",
              "      <th>-0.055685</th>\n",
              "      <th>-0.087141</th>\n",
              "      <th>-0.089251</th>\n",
              "      <th>-0.073396</th>\n",
              "      <th>-0.058741</th>\n",
              "      <th>-0.06799</th>\n",
              "      <th>-0.038718</th>\n",
              "      <th>-0.097449</th>\n",
              "      <th>-0.087578</th>\n",
              "      <th>-0.090969</th>\n",
              "      <th>-0.072003</th>\n",
              "      <th>-0.081238</th>\n",
              "      <th>-0.081747</th>\n",
              "      <th>-0.068767</th>\n",
              "      <th>-0.087579</th>\n",
              "      <th>-0.075662</th>\n",
              "      <th>-0.082675</th>\n",
              "      <th>-0.07205</th>\n",
              "      <th>-0.068462</th>\n",
              "      <th>-0.045573</th>\n",
              "      <th>-0.073261</th>\n",
              "      <th>-0.053124</th>\n",
              "      <th>-0.06232</th>\n",
              "      <th>-0.04988</th>\n",
              "      <th>-0.0506</th>\n",
              "      <th>-0.105394</th>\n",
              "      <th>-0.090395</th>\n",
              "      <th>-0.099816</th>\n",
              "      <th>-0.102888</th>\n",
              "      <th>-0.102903</th>\n",
              "      <th>-0.090115</th>\n",
              "      <th>-0.096068</th>\n",
              "      <th>-0.068136</th>\n",
              "      <th>-0.060835</th>\n",
              "      <th>-0.058394</th>\n",
              "      <td>-0.062079</td>\n",
              "      <td>-0.044674</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.041448</td>\n",
              "      <td>-0.038775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <th>2</th>\n",
              "      <th>131.0</th>\n",
              "      <th>0</th>\n",
              "      <th>-0.175063</th>\n",
              "      <th>-0.176678</th>\n",
              "      <th>-0.937091</th>\n",
              "      <th>-0.381926</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.636535</th>\n",
              "      <th>1.052396</th>\n",
              "      <th>-0.16938</th>\n",
              "      <th>-0.19304</th>\n",
              "      <th>-0.169234</th>\n",
              "      <th>-0.828942</th>\n",
              "      <th>-0.175093</th>\n",
              "      <th>-0.239675</th>\n",
              "      <th>-0.081759</th>\n",
              "      <th>-0.438492</th>\n",
              "      <th>-0.265227</th>\n",
              "      <th>-0.270534</th>\n",
              "      <th>-0.766241</th>\n",
              "      <th>-0.230818</th>\n",
              "      <th>-0.208397</th>\n",
              "      <th>-0.031458</th>\n",
              "      <th>-0.146052</th>\n",
              "      <th>-0.125919</th>\n",
              "      <th>-0.114999</th>\n",
              "      <th>-0.073666</th>\n",
              "      <th>-0.080377</th>\n",
              "      <th>-0.039977</th>\n",
              "      <th>-0.086417</th>\n",
              "      <th>-0.07646</th>\n",
              "      <th>-0.115336</th>\n",
              "      <th>-0.112976</th>\n",
              "      <th>-0.129556</th>\n",
              "      <th>-0.114021</th>\n",
              "      <th>-0.126911</th>\n",
              "      <th>-0.112939</th>\n",
              "      <th>-0.08632</th>\n",
              "      <th>-0.12648</th>\n",
              "      <th>-0.140237</th>\n",
              "      <th>-0.127462</th>\n",
              "      <th>-0.101234</th>\n",
              "      <th>-0.12889</th>\n",
              "      <th>-0.12375</th>\n",
              "      <th>-0.036678</th>\n",
              "      <th>-0.132163</th>\n",
              "      <th>-0.091828</th>\n",
              "      <th>-0.076866</th>\n",
              "      <th>-0.083422</th>\n",
              "      <th>-0.132653</th>\n",
              "      <th>-0.132271</th>\n",
              "      <th>-0.113523</th>\n",
              "      <th>-0.082894</th>\n",
              "      <th>-0.100532</th>\n",
              "      <th>-0.075784</th>\n",
              "      <th>-0.143635</th>\n",
              "      <th>-0.127528</th>\n",
              "      <th>-0.128455</th>\n",
              "      <th>-0.099046</th>\n",
              "      <th>-0.112457</th>\n",
              "      <th>-0.118284</th>\n",
              "      <th>-0.105322</th>\n",
              "      <th>-0.117228</th>\n",
              "      <th>-0.105303</th>\n",
              "      <th>-0.130077</th>\n",
              "      <th>-0.117335</th>\n",
              "      <th>-0.100452</th>\n",
              "      <th>-0.082987</th>\n",
              "      <th>-0.109775</th>\n",
              "      <th>-0.082519</th>\n",
              "      <th>-0.09015</th>\n",
              "      <th>-0.061807</th>\n",
              "      <th>-0.076595</th>\n",
              "      <th>-0.154405</th>\n",
              "      <th>-0.133709</th>\n",
              "      <th>-0.149318</th>\n",
              "      <th>-0.147526</th>\n",
              "      <th>-0.157631</th>\n",
              "      <th>-0.133908</th>\n",
              "      <th>-0.142128</th>\n",
              "      <th>-0.101243</th>\n",
              "      <th>-0.094235</th>\n",
              "      <th>-0.084585</th>\n",
              "      <th>-0.089205</th>\n",
              "      <th>-0.05786</th>\n",
              "      <th>-0.067859</th>\n",
              "      <th>-0.055429</th>\n",
              "      <th>-0.052329</th>\n",
              "      <th>0.103617</th>\n",
              "      <th>0.103647</th>\n",
              "      <th>1.795261</th>\n",
              "      <th>-0.289939</th>\n",
              "      <th>0.0</th>\n",
              "      <th>1.504898</th>\n",
              "      <th>-1.638999</th>\n",
              "      <th>0.011080</th>\n",
              "      <th>0.048542</th>\n",
              "      <th>0.234983</th>\n",
              "      <th>2.280152</th>\n",
              "      <th>-0.120957</th>\n",
              "      <th>-0.175118</th>\n",
              "      <th>-0.062041</th>\n",
              "      <th>-0.265922</th>\n",
              "      <th>0.203427</th>\n",
              "      <th>0.074122</th>\n",
              "      <th>1.595160</th>\n",
              "      <th>-0.151928</th>\n",
              "      <th>-0.143451</th>\n",
              "      <th>-0.009262</th>\n",
              "      <th>-0.103378</th>\n",
              "      <th>-0.089099</th>\n",
              "      <th>-0.081998</th>\n",
              "      <th>-0.052605</th>\n",
              "      <th>-0.0557</th>\n",
              "      <th>-0.030846</th>\n",
              "      <th>-0.060957</th>\n",
              "      <th>-0.053846</th>\n",
              "      <th>-0.081624</th>\n",
              "      <th>-0.078441</th>\n",
              "      <th>-0.087999</th>\n",
              "      <th>-0.079429</th>\n",
              "      <th>-0.088813</th>\n",
              "      <th>-0.071986</th>\n",
              "      <th>-0.065387</th>\n",
              "      <th>-0.089995</th>\n",
              "      <th>-0.097408</th>\n",
              "      <th>-0.093064</th>\n",
              "      <th>-0.076343</th>\n",
              "      <th>-0.086655</th>\n",
              "      <th>-0.085351</th>\n",
              "      <th>-0.036412</th>\n",
              "      <th>-0.087365</th>\n",
              "      <th>-0.059447</th>\n",
              "      <th>-0.053389</th>\n",
              "      <th>-0.055685</th>\n",
              "      <th>-0.087141</th>\n",
              "      <th>-0.089251</th>\n",
              "      <th>-0.073396</th>\n",
              "      <th>-0.058741</th>\n",
              "      <th>-0.06799</th>\n",
              "      <th>-0.038718</th>\n",
              "      <th>-0.097449</th>\n",
              "      <th>-0.087578</th>\n",
              "      <th>-0.090969</th>\n",
              "      <th>-0.072003</th>\n",
              "      <th>-0.081238</th>\n",
              "      <th>-0.081747</th>\n",
              "      <th>-0.068767</th>\n",
              "      <th>-0.087579</th>\n",
              "      <th>-0.075662</th>\n",
              "      <th>-0.082675</th>\n",
              "      <th>-0.07205</th>\n",
              "      <th>-0.068462</th>\n",
              "      <th>-0.045573</th>\n",
              "      <th>-0.073261</th>\n",
              "      <th>-0.053124</th>\n",
              "      <th>-0.06232</th>\n",
              "      <th>-0.04988</th>\n",
              "      <th>-0.0506</th>\n",
              "      <th>-0.105394</th>\n",
              "      <th>-0.090395</th>\n",
              "      <th>-0.099816</th>\n",
              "      <th>-0.102888</th>\n",
              "      <th>-0.102903</th>\n",
              "      <th>-0.090115</th>\n",
              "      <th>-0.096068</th>\n",
              "      <th>-0.068136</th>\n",
              "      <th>-0.060835</th>\n",
              "      <th>-0.058394</th>\n",
              "      <td>-0.062079</td>\n",
              "      <td>-0.044674</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.041448</td>\n",
              "      <td>-0.038775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>1</th>\n",
              "      <th>150.0</th>\n",
              "      <th>0</th>\n",
              "      <th>-0.175063</th>\n",
              "      <th>-0.176678</th>\n",
              "      <th>-0.937091</th>\n",
              "      <th>-0.381926</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.636535</th>\n",
              "      <th>1.052396</th>\n",
              "      <th>-0.16938</th>\n",
              "      <th>-0.19304</th>\n",
              "      <th>-0.169234</th>\n",
              "      <th>-0.828942</th>\n",
              "      <th>-0.175093</th>\n",
              "      <th>-0.239675</th>\n",
              "      <th>-0.081759</th>\n",
              "      <th>-0.438492</th>\n",
              "      <th>-0.265227</th>\n",
              "      <th>-0.270534</th>\n",
              "      <th>-0.766241</th>\n",
              "      <th>-0.230818</th>\n",
              "      <th>-0.208397</th>\n",
              "      <th>-0.031458</th>\n",
              "      <th>-0.146052</th>\n",
              "      <th>-0.125919</th>\n",
              "      <th>-0.114999</th>\n",
              "      <th>-0.073666</th>\n",
              "      <th>-0.080377</th>\n",
              "      <th>-0.039977</th>\n",
              "      <th>-0.086417</th>\n",
              "      <th>-0.07646</th>\n",
              "      <th>-0.115336</th>\n",
              "      <th>-0.112976</th>\n",
              "      <th>-0.129556</th>\n",
              "      <th>-0.114021</th>\n",
              "      <th>-0.126911</th>\n",
              "      <th>-0.112939</th>\n",
              "      <th>-0.08632</th>\n",
              "      <th>-0.12648</th>\n",
              "      <th>-0.140237</th>\n",
              "      <th>-0.127462</th>\n",
              "      <th>-0.101234</th>\n",
              "      <th>-0.12889</th>\n",
              "      <th>-0.12375</th>\n",
              "      <th>-0.036678</th>\n",
              "      <th>-0.132163</th>\n",
              "      <th>-0.091828</th>\n",
              "      <th>-0.076866</th>\n",
              "      <th>-0.083422</th>\n",
              "      <th>-0.132653</th>\n",
              "      <th>-0.132271</th>\n",
              "      <th>-0.113523</th>\n",
              "      <th>-0.082894</th>\n",
              "      <th>-0.100532</th>\n",
              "      <th>-0.075784</th>\n",
              "      <th>-0.143635</th>\n",
              "      <th>-0.127528</th>\n",
              "      <th>-0.128455</th>\n",
              "      <th>-0.099046</th>\n",
              "      <th>-0.112457</th>\n",
              "      <th>-0.118284</th>\n",
              "      <th>-0.105322</th>\n",
              "      <th>-0.117228</th>\n",
              "      <th>-0.105303</th>\n",
              "      <th>-0.130077</th>\n",
              "      <th>-0.117335</th>\n",
              "      <th>-0.100452</th>\n",
              "      <th>-0.082987</th>\n",
              "      <th>-0.109775</th>\n",
              "      <th>-0.082519</th>\n",
              "      <th>-0.09015</th>\n",
              "      <th>-0.061807</th>\n",
              "      <th>-0.076595</th>\n",
              "      <th>-0.154405</th>\n",
              "      <th>-0.133709</th>\n",
              "      <th>-0.149318</th>\n",
              "      <th>-0.147526</th>\n",
              "      <th>-0.157631</th>\n",
              "      <th>-0.133908</th>\n",
              "      <th>-0.142128</th>\n",
              "      <th>-0.101243</th>\n",
              "      <th>-0.094235</th>\n",
              "      <th>-0.084585</th>\n",
              "      <th>-0.089205</th>\n",
              "      <th>-0.05786</th>\n",
              "      <th>-0.067859</th>\n",
              "      <th>-0.055429</th>\n",
              "      <th>-0.052329</th>\n",
              "      <th>-0.114147</th>\n",
              "      <th>-0.115286</th>\n",
              "      <th>-0.649615</th>\n",
              "      <th>-0.289939</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.417990</th>\n",
              "      <th>0.723261</th>\n",
              "      <th>-0.109215</th>\n",
              "      <th>-0.122009</th>\n",
              "      <th>-0.109234</th>\n",
              "      <th>-0.559786</th>\n",
              "      <th>-0.120957</th>\n",
              "      <th>-0.175118</th>\n",
              "      <th>-0.062041</th>\n",
              "      <th>-0.265922</th>\n",
              "      <th>-0.155394</th>\n",
              "      <th>-0.157044</th>\n",
              "      <th>-0.515898</th>\n",
              "      <th>-0.151928</th>\n",
              "      <th>-0.143451</th>\n",
              "      <th>-0.009262</th>\n",
              "      <th>-0.103378</th>\n",
              "      <th>-0.089099</th>\n",
              "      <th>-0.081998</th>\n",
              "      <th>-0.052605</th>\n",
              "      <th>-0.0557</th>\n",
              "      <th>-0.030846</th>\n",
              "      <th>-0.060957</th>\n",
              "      <th>-0.053846</th>\n",
              "      <th>-0.081624</th>\n",
              "      <th>-0.078441</th>\n",
              "      <th>-0.087999</th>\n",
              "      <th>-0.079429</th>\n",
              "      <th>-0.088813</th>\n",
              "      <th>-0.071986</th>\n",
              "      <th>-0.065387</th>\n",
              "      <th>-0.089995</th>\n",
              "      <th>-0.097408</th>\n",
              "      <th>-0.093064</th>\n",
              "      <th>-0.076343</th>\n",
              "      <th>-0.086655</th>\n",
              "      <th>-0.085351</th>\n",
              "      <th>-0.036412</th>\n",
              "      <th>-0.087365</th>\n",
              "      <th>-0.059447</th>\n",
              "      <th>-0.053389</th>\n",
              "      <th>-0.055685</th>\n",
              "      <th>-0.087141</th>\n",
              "      <th>-0.089251</th>\n",
              "      <th>-0.073396</th>\n",
              "      <th>-0.058741</th>\n",
              "      <th>-0.06799</th>\n",
              "      <th>-0.038718</th>\n",
              "      <th>-0.097449</th>\n",
              "      <th>-0.087578</th>\n",
              "      <th>-0.090969</th>\n",
              "      <th>-0.072003</th>\n",
              "      <th>-0.081238</th>\n",
              "      <th>-0.081747</th>\n",
              "      <th>-0.068767</th>\n",
              "      <th>-0.087579</th>\n",
              "      <th>-0.075662</th>\n",
              "      <th>-0.082675</th>\n",
              "      <th>-0.07205</th>\n",
              "      <th>-0.068462</th>\n",
              "      <th>-0.045573</th>\n",
              "      <th>-0.073261</th>\n",
              "      <th>-0.053124</th>\n",
              "      <th>-0.06232</th>\n",
              "      <th>-0.04988</th>\n",
              "      <th>-0.0506</th>\n",
              "      <th>-0.105394</th>\n",
              "      <th>-0.090395</th>\n",
              "      <th>-0.099816</th>\n",
              "      <th>-0.102888</th>\n",
              "      <th>-0.102903</th>\n",
              "      <th>-0.090115</th>\n",
              "      <th>-0.096068</th>\n",
              "      <th>-0.068136</th>\n",
              "      <th>-0.060835</th>\n",
              "      <th>-0.058394</th>\n",
              "      <td>-0.062079</td>\n",
              "      <td>-0.044674</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.041448</td>\n",
              "      <td>-0.038775</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-792f24c1-bff3-484a-b7d3-97f4a881cb40')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-792f24c1-bff3-484a-b7d3-97f4a881cb40 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-792f24c1-bff3-484a-b7d3-97f4a881cb40');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4b6bb334-7f17-4edc-8018-3d108ea3dbdc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b6bb334-7f17-4edc-8018-3d108ea3dbdc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4b6bb334-7f17-4edc-8018-3d108ea3dbdc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      user_id  \\\n",
              "0 0 0.0   0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.085588 -0.084010  1.756454 -0.289939 0.0 -0.417990 -1.410948 -0.109215 -0.122009 -0.040390  1.825762 -0.120957 -0.175118 -0.062041 -0.265922 -0.108335 -0.118516  1.196213 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.062079   \n",
              "1 1 36.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.062079   \n",
              "    77.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.062079   \n",
              "2 2 131.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329  0.103617  0.103647  1.795261 -0.289939 0.0  1.504898 -1.638999  0.011080  0.048542  0.234983  2.280152 -0.120957 -0.175118 -0.062041 -0.265922  0.203427  0.074122  1.595160 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.062079   \n",
              "1 1 150.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.062079   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      item_id  \\\n",
              "0 0 0.0   0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.085588 -0.084010  1.756454 -0.289939 0.0 -0.417990 -1.410948 -0.109215 -0.122009 -0.040390  1.825762 -0.120957 -0.175118 -0.062041 -0.265922 -0.108335 -0.118516  1.196213 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.044674   \n",
              "1 1 36.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.044674   \n",
              "    77.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.044674   \n",
              "2 2 131.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329  0.103617  0.103647  1.795261 -0.289939 0.0  1.504898 -1.638999  0.011080  0.048542  0.234983  2.280152 -0.120957 -0.175118 -0.062041 -0.265922  0.203427  0.074122  1.595160 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.044674   \n",
              "1 1 150.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394 -0.044674   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     timestamp  \\\n",
              "0 0 0.0   0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.085588 -0.084010  1.756454 -0.289939 0.0 -0.417990 -1.410948 -0.109215 -0.122009 -0.040390  1.825762 -0.120957 -0.175118 -0.062041 -0.265922 -0.108335 -0.118516  1.196213 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394  -0.050464   \n",
              "1 1 36.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394  -0.050464   \n",
              "    77.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394  -0.050464   \n",
              "2 2 131.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329  0.103617  0.103647  1.795261 -0.289939 0.0  1.504898 -1.638999  0.011080  0.048542  0.234983  2.280152 -0.120957 -0.175118 -0.062041 -0.265922  0.203427  0.074122  1.595160 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394  -0.050464   \n",
              "1 1 150.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394  -0.050464   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     state_label  \\\n",
              "0 0 0.0   0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.085588 -0.084010  1.756454 -0.289939 0.0 -0.417990 -1.410948 -0.109215 -0.122009 -0.040390  1.825762 -0.120957 -0.175118 -0.062041 -0.265922 -0.108335 -0.118516  1.196213 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394    -0.041448   \n",
              "1 1 36.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394    -0.041448   \n",
              "    77.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394    -0.041448   \n",
              "2 2 131.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329  0.103617  0.103647  1.795261 -0.289939 0.0  1.504898 -1.638999  0.011080  0.048542  0.234983  2.280152 -0.120957 -0.175118 -0.062041 -0.265922  0.203427  0.074122  1.595160 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394    -0.041448   \n",
              "1 1 150.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394    -0.041448   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     comma_separated_list_of_features  \n",
              "0 0 0.0   0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.085588 -0.084010  1.756454 -0.289939 0.0 -0.417990 -1.410948 -0.109215 -0.122009 -0.040390  1.825762 -0.120957 -0.175118 -0.062041 -0.265922 -0.108335 -0.118516  1.196213 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394                         -0.038775  \n",
              "1 1 36.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394                         -0.038775  \n",
              "    77.0  0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394                         -0.038775  \n",
              "2 2 131.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329  0.103617  0.103647  1.795261 -0.289939 0.0  1.504898 -1.638999  0.011080  0.048542  0.234983  2.280152 -0.120957 -0.175118 -0.062041 -0.265922  0.203427  0.074122  1.595160 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394                         -0.038775  \n",
              "1 1 150.0 0 -0.175063 -0.176678 -0.937091 -0.381926 0.0 -0.636535 1.052396 -0.16938 -0.19304 -0.169234 -0.828942 -0.175093 -0.239675 -0.081759 -0.438492 -0.265227 -0.270534 -0.766241 -0.230818 -0.208397 -0.031458 -0.146052 -0.125919 -0.114999 -0.073666 -0.080377 -0.039977 -0.086417 -0.07646 -0.115336 -0.112976 -0.129556 -0.114021 -0.126911 -0.112939 -0.08632 -0.12648 -0.140237 -0.127462 -0.101234 -0.12889 -0.12375 -0.036678 -0.132163 -0.091828 -0.076866 -0.083422 -0.132653 -0.132271 -0.113523 -0.082894 -0.100532 -0.075784 -0.143635 -0.127528 -0.128455 -0.099046 -0.112457 -0.118284 -0.105322 -0.117228 -0.105303 -0.130077 -0.117335 -0.100452 -0.082987 -0.109775 -0.082519 -0.09015 -0.061807 -0.076595 -0.154405 -0.133709 -0.149318 -0.147526 -0.157631 -0.133908 -0.142128 -0.101243 -0.094235 -0.084585 -0.089205 -0.05786 -0.067859 -0.055429 -0.052329 -0.114147 -0.115286 -0.649615 -0.289939 0.0 -0.417990  0.723261 -0.109215 -0.122009 -0.109234 -0.559786 -0.120957 -0.175118 -0.062041 -0.265922 -0.155394 -0.157044 -0.515898 -0.151928 -0.143451 -0.009262 -0.103378 -0.089099 -0.081998 -0.052605 -0.0557 -0.030846 -0.060957 -0.053846 -0.081624 -0.078441 -0.087999 -0.079429 -0.088813 -0.071986 -0.065387 -0.089995 -0.097408 -0.093064 -0.076343 -0.086655 -0.085351 -0.036412 -0.087365 -0.059447 -0.053389 -0.055685 -0.087141 -0.089251 -0.073396 -0.058741 -0.06799 -0.038718 -0.097449 -0.087578 -0.090969 -0.072003 -0.081238 -0.081747 -0.068767 -0.087579 -0.075662 -0.082675 -0.07205 -0.068462 -0.045573 -0.073261 -0.053124 -0.06232 -0.04988 -0.0506 -0.105394 -0.090395 -0.099816 -0.102888 -0.102903 -0.090115 -0.096068 -0.068136 -0.060835 -0.058394                         -0.038775  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data/wikipedia.csv')\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmj5a2ptKjI7",
        "outputId": "0310623c-df38-45b4-bd5f-605b89238dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['user_id', 'item_id', 'timestamp', 'state_label',\n",
            "       'comma_separated_list_of_features'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "column_names = data.columns\n",
        "print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M34MUCmhJZ9_",
        "outputId": "1dc581fe-7e40-492b-901e-8b349b41b661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp (from torch-geometric)\n",
            "  Downloading aiohttp-3.11.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch-geometric)\n",
            "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->torch-geometric)\n",
            "  Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch-geometric)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: propcache, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch-geometric\n",
            "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 frozenlist-1.5.0 propcache-0.2.1 torch-geometric-2.6.1 yarl-1.18.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRmAs5GNbASo"
      },
      "source": [
        "# **This piece of code tries to check the best hyper-parameter for our model and also compares each of them to see what best suits our needs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "opmsINqzL7ye",
        "outputId": "642244b9-0a37-4975-a9e7-9368781ed575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing combination: hidden_dim=16, heads=8, lr=0.01, dropout=0.3, weight_decay=0.0\n",
            "Accuracy for this combination: 0.5192307692307693\n",
            "Testing combination: hidden_dim=16, heads=8, lr=0.01, dropout=0.3, weight_decay=0.001\n",
            "Accuracy for this combination: 0.4423076923076923\n",
            "Testing combination: hidden_dim=16, heads=8, lr=0.01, dropout=0.5, weight_decay=0.0\n",
            "Accuracy for this combination: 0.5192307692307693\n",
            "Testing combination: hidden_dim=16, heads=8, lr=0.01, dropout=0.5, weight_decay=0.001\n",
            "Accuracy for this combination: 0.5769230769230769\n",
            "Testing combination: hidden_dim=16, heads=8, lr=0.001, dropout=0.3, weight_decay=0.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a53cd586847a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Train for fewer epochs to save time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a53cd586847a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         alpha = self.edge_updater(edge_index, alpha=alpha, edge_attr=edge_attr,\n\u001b[0m\u001b[1;32m    363\u001b[0m                                   size=size)\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torch_geometric.nn.conv.gat_conv_GATConv_edge_updater_j9x_7vvd.py\u001b[0m in \u001b[0;36medge_updater\u001b[0;34m(self, edge_index, alpha, edge_attr, size)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# End Edge Update Forward Pre Hook #########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     out = self.edge_update(\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0malpha_j\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_j\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0malpha_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36medge_update\u001b[0;34m(self, alpha_j, alpha_i, edge_attr, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_softmax.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(src, index, ptr, num_nodes, dim)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_num_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0msrc_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msrc_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_in_onnx_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     return src.new_zeros(size).scatter_reduce_(\n\u001b[0m\u001b[1;32m    103\u001b[0m                         \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'a{reduce[-3:]}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         include_self=False)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_dims = [16, 32]\n",
        "heads = [8, 16]\n",
        "learning_rates = [0.01, 0.001]\n",
        "dropouts = [0.3, 0.5]\n",
        "weight_decays = [0.0, 0.001]\n",
        "\n",
        "# Create all combinations of hyperparameters\n",
        "param_grid = list(product(hidden_dims, heads, learning_rates, dropouts, weight_decays))\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "# Iterate over all combinations\n",
        "for hidden_dim, head, lr, dropout, weight_decay in param_grid:\n",
        "    print(f\"Testing combination: hidden_dim={hidden_dim}, heads={head}, lr={lr}, dropout={dropout}, weight_decay={weight_decay}\")\n",
        "\n",
        "    # Define the model\n",
        "    class GAT(torch.nn.Module):\n",
        "        def __init__(self, input_dim, hidden_dim, output_dim, heads, dropout):\n",
        "            super(GAT, self).__init__()\n",
        "            self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, dropout=dropout)\n",
        "            self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
        "\n",
        "        def forward(self, data):\n",
        "            x, edge_index = data.x, data.edge_index\n",
        "            x = self.conv1(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = self.conv2(x, edge_index)\n",
        "            return F.log_softmax(x, dim=1)\n",
        "\n",
        "    model = GAT(input_dim=data.num_node_features, hidden_dim=hidden_dim, output_dim=2, heads=head, dropout=dropout)\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for epoch in range(50):  # Train for fewer epochs to save time\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[train_mask], data.y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    _, pred = model(data).max(dim=1)\n",
        "    correct = (pred[test_mask] == data.y[test_mask]).sum()\n",
        "    accuracy = int(correct) / int(test_mask.sum())\n",
        "\n",
        "    print(f\"Accuracy for this combination: {accuracy}\")\n",
        "\n",
        "    # Track best accuracy and parameters\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_params = (hidden_dim, head, lr, dropout, weight_decay)\n",
        "\n",
        "print(f\"Best accuracy: {best_accuracy}\")\n",
        "print(f\"Best parameters: hidden_dim={best_params[0]}, heads={best_params[1]}, lr={best_params[2]}, dropout={best_params[3]}, weight_decay={best_params[4]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDf-oUu5bTot"
      },
      "source": [
        "# **Check the Evaluation Mettrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFGtbnyjdQZB",
        "outputId": "97399c88-f809-4774-9594-1341632c7154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall: 0.5000\n",
            "Mean Reciprocal Rank (MRR): 0.7750\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Get raw predictions (logits)\n",
        "out = model(data)\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "probs = torch.softmax(out, dim=1)\n",
        "\n",
        "# Get ranked positions\n",
        "_, ranked_indices = probs.sort(dim=1, descending=True)  # Sort predictions in descending order\n",
        "\n",
        "# Select only test samples\n",
        "test_indices = test_mask.nonzero(as_tuple=True)[0]  # Indices of test nodes\n",
        "true_labels = data.y[test_indices]  # True labels of test nodes\n",
        "ranked_indices = ranked_indices[test_indices]  # Ranked predictions for test nodes\n",
        "\n",
        "# Compute Recall (Top-1)\n",
        "y_pred = ranked_indices[:, 0]  # Take top-1 prediction\n",
        "recall = recall_score(true_labels.cpu(), y_pred.cpu(), average='macro')  # Macro Recall\n",
        "\n",
        "# Compute Mean Reciprocal Rank (MRR)\n",
        "ranks = (ranked_indices == true_labels.view(-1, 1)).nonzero(as_tuple=True)[1] + 1  # Get rank positions (1-based)\n",
        "mrr = (1.0 / ranks.float()).mean().item()\n",
        "\n",
        "# Print results\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U82RGUDGnux5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRPxCwaIbb_S"
      },
      "source": [
        "# **The final code for GAT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4f1wZ4qsLNg",
        "outputId": "4f72efd7-255c-4319-b580-6a15123d6f3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-b1aa43ac28bf>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  edge_list[\"source\"] = edge_list[\"source\"].map(node_id_mapping)\n",
            "<ipython-input-16-b1aa43ac28bf>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  edge_list[\"target\"] = edge_list[\"target\"].map(node_id_mapping)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph Data Object: Data(x=[288, 1], edge_index=[2, 672447], edge_attr=[672447], y=[288])\n",
            "GAT Model Initialized!\n",
            "Epoch 1, Loss: 0.6948347687721252\n",
            "Epoch 11, Loss: 0.6889665722846985\n",
            "Epoch 21, Loss: 0.6897634863853455\n",
            "Epoch 31, Loss: 0.6930219531059265\n",
            "Epoch 41, Loss: 0.6880554556846619\n",
            "Training Complete!\n",
            "Recall@10: 1.0000\n",
            "Mean Reciprocal Rank (MRR): 0.7453\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "# Step 1: Load and Preprocess Dataset\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"data/reddit.csv\")\n",
        "\n",
        "edge_list = data[[\"user_id\", \"item_id\", \"timestamp\", \"state_label\"]]\n",
        "edge_list.columns = [\"source\", \"target\", \"timestamp\", \"interaction_feature\"]\n",
        "\n",
        "user_features = data[[\"user_id\", \"comma_separated_list_of_features\"]].drop_duplicates()\n",
        "user_features.columns = [\"node_id\", \"features\"]\n",
        "\n",
        "item_features = data[[\"item_id\", \"comma_separated_list_of_features\"]].drop_duplicates()\n",
        "item_features.columns = [\"node_id\", \"features\"]\n",
        "\n",
        "node_features = pd.concat([user_features, item_features]).drop_duplicates()\n",
        "all_node_ids = pd.concat([edge_list[\"source\"], edge_list[\"target\"]]).unique()\n",
        "node_features = node_features[node_features[\"node_id\"].isin(all_node_ids)]\n",
        "\n",
        "node_id_mapping = {node_id: i for i, node_id in enumerate(node_features[\"node_id\"].unique())}\n",
        "edge_list[\"source\"] = edge_list[\"source\"].map(node_id_mapping)\n",
        "edge_list[\"target\"] = edge_list[\"target\"].map(node_id_mapping)\n",
        "node_features[\"node_id\"] = node_features[\"node_id\"].map(node_id_mapping)\n",
        "\n",
        "\n",
        "# Step 2: Convert Data into PyTorch Geometric Format\n",
        "\n",
        "scaler = StandardScaler()\n",
        "node_features_matrix = node_features[\"features\"].fillna(\"\").apply(\n",
        "    lambda x: list(map(float, str(x).split(\",\"))) if x != \"\" else [0.0] * 171\n",
        ").tolist()\n",
        "node_features_matrix = scaler.fit_transform(node_features_matrix)\n",
        "x = torch.tensor(node_features_matrix, dtype=torch.float)\n",
        "\n",
        "edge_index = torch.tensor(edge_list[[\"source\", \"target\"]].values.T, dtype=torch.long)\n",
        "\n",
        "edge_weights = torch.tensor(edge_list[\"interaction_feature\"].values, dtype=torch.float)\n",
        "\n",
        "neg_edges = negative_sampling(edge_index, num_nodes=x.size(0), num_neg_samples=edge_index.size(1))\n",
        "edge_index = torch.cat([edge_index, neg_edges], dim=1)\n",
        "\n",
        "labels = torch.randint(0, 2, (x.size(0),))  # Binary classification\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights, y=labels)\n",
        "print(f\"Graph Data Object: {data}\")\n",
        "\n",
        "# Step 3: GAT Model\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True, edge_dim=1)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, edge_dim=1)\n",
        "        self.conv3 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, edge_dim=1)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index, edge_attr)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "input_dim = data.num_node_features\n",
        "hidden_dim = 128\n",
        "output_dim = 2\n",
        "model = GAT(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(\"GAT Model Initialized!\")\n",
        "\n",
        "# Training The Model\n",
        "train_mask = torch.rand(len(data.y)) < 0.8\n",
        "test_mask = ~train_mask\n",
        "\n",
        "class_counts = torch.bincount(data.y)\n",
        "weights = 1.0 / class_counts.float()\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(50):  # Increased epochs\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out[train_mask], data.y[train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "print(\"Training Complete!\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "out = model(data)\n",
        "_, ranked_indices = out[test_mask].sort(descending=True, dim=1)\n",
        "true_labels = data.y[test_mask]\n",
        "\n",
        "# Compute Recall@10\n",
        "top_k = 10\n",
        "recall_at_k = (true_labels.view(-1, 1) == ranked_indices[:, :top_k]).sum().item() / len(true_labels)\n",
        "\n",
        "ranks = (ranked_indices == true_labels.view(-1, 1)).nonzero(as_tuple=True)[1] + 1\n",
        "mrr = (1.0 / ranks.float()).mean().item()\n",
        "\n",
        "print(f\"Recall@{top_k}: {recall_at_k:.4f}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nH7n0V07j6n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
